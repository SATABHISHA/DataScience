{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocess all the Data we have in DonorsChoose Dataset. \n",
    "2. Combine total Text into one column named - 'preprocessed_essays'. \n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use 'auc' as a metric. \n",
    "5. You are free to choose no of layers and no of hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum. \n",
    "7. For all the model's use TensorBoard and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in .ipynb notebook and PDF. \n",
    "8. Use Categorical Cross Entropy as Loss to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-1\n",
    "\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](model-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_seq_total_text_data --- You have to give Total test data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- input_school_state --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- project_grade_category  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- input_clean_categories --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- input_clean_subcategories --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- input_clean_subcategories --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- input_remaining_teacher_number_of_previously_posted_projects___resource_summary_contains_numerical_digits___price__quantity - Concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For LSTM, you can choose your sequence padding methods on your own or you can train your LSTM without padding, there is no restriction on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(n,))\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "flatten = Flatten()(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the TF-IDF on the Train data\n",
    "2. Get the Idf value for each word we have in the train data. \n",
    "3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very freuent words and very very rare words don't give much information.\n",
    "4. Train the LSTM after removing the Low and High idf value words. ( In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](lstm_conv-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_seq_total_text_data --- You have to give Total test data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. You are free to preprocess the input text as you needed. \n",
    "- Otherthan_text_data --- Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors and Neumerical values and use CNN1D as shown in above figure. You are free to choose all CNN parameters like kernel sizes, stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
